"""
JSON Kaydedici - OCR sonu√ßlarƒ±nƒ± JSON formatƒ±nda yazar

OCR sonu√ßlarƒ±nƒ± zaman bilgisi ve konum verileriyle birlikte
yapƒ±landƒ±rƒ±lmƒ±≈ü JSON formatƒ±nda kaydeder.
"""

import json
import os
from typing import List, Dict, Any, Tuple
from datetime import datetime

OUTPUT_DIR = r"C:\Users\ASUS\Desktop\videotext\processor\output"

class JsonWriter:
    """
    OCR sonu√ßlarƒ±nƒ± JSON formatƒ±nda kaydeder
    
    √ñzellikler:
    - UTF-8 desteƒüi (T√ºrk√ße karakterler)
    - Yapƒ±landƒ±rƒ±lmƒ±≈ü veri formatƒ±
    - Metadata bilgileri
    - Performans optimizasyonu
    """
    
    def __init__(self, output_path: str):
        """
        Args:
            output_path (str): √áƒ±ktƒ± JSON dosyasƒ± yolu
        """
        self.output_path = output_path
        self.data: List[Dict[str, Any]] = []
        self.metadata = {
            "created_at": datetime.now().isoformat(),
            "total_detections": 0,
            "processing_info": {}
        }
        
        # √áƒ±ktƒ± dizinini olu≈ütur
        os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
    
    def add(self, timestamp: float, bbox: Tuple[float, float, float, float], 
            text: str, confidence: float):
        """
        OCR sonucunu veri listesine ekler
        
        Args:
            timestamp (float): Saniye cinsinden zaman
            bbox (tuple): Sƒ±nƒ±rlayƒ±cƒ± kutu koordinatlarƒ± (x1, y1, x2, y2)
            text (str): Tanƒ±nan metin
            confidence (float): G√ºven skoru (0.0-1.0)
        """
        x1, y1, x2, y2 = bbox
        
        detection = {
            "timestamp": round(timestamp, 3),
            "bbox": {
                "x1": int(x1),
                "y1": int(y1),
                "x2": int(x2),
                "y2": int(y2),
                "width": int(x2 - x1),
                "height": int(y2 - y1)
            },
            "text": text.strip(),
            "confidence": round(confidence, 4),
            "text_length": len(text.strip()),
            "detection_id": len(self.data) + 1
        }
        
        self.data.append(detection)
        self.metadata["total_detections"] += 1
    
    def add_metadata(self, key: str, value: Any):
        """
        Metadata bilgisi ekler
        
        Args:
            key (str): Metadata anahtarƒ±
            value (Any): Metadata deƒüeri
        """
        self.metadata["processing_info"][key] = value
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ƒ∞≈ülem istatistiklerini d√∂ner
        
        Returns:
            dict: ƒ∞statistik bilgileri
        """
        if not self.data:
            return {"message": "Hen√ºz veri yok"}
        
        confidences = [d["confidence"] for d in self.data]
        text_lengths = [d["text_length"] for d in self.data]
        timestamps = [d["timestamp"] for d in self.data]
        
        stats = {
            "total_detections": len(self.data),
            "confidence_stats": {
                "min": min(confidences),
                "max": max(confidences),
                "avg": sum(confidences) / len(confidences)
            },
            "text_length_stats": {
                "min": min(text_lengths),
                "max": max(text_lengths),
                "avg": sum(text_lengths) / len(text_lengths)
            },
            "time_range": {
                "start": min(timestamps),
                "end": max(timestamps),
                "duration": max(timestamps) - min(timestamps)
            },
            "high_confidence_count": len([c for c in confidences if c > 0.8]),
            "unique_texts": len(set(d["text"] for d in self.data))
        }
        
        return stats
    
    def filter_by_confidence(self, min_confidence: float = 0.5):
        """
        D√º≈ü√ºk g√ºven skorlu sonu√ßlarƒ± filtreler
        
        Args:
            min_confidence (float): Minimum g√ºven skoru
        """
        original_count = len(self.data)
        self.data = [d for d in self.data if d["confidence"] >= min_confidence]
        filtered_count = original_count - len(self.data)
        
        if filtered_count > 0:
            print(f"üîç {filtered_count} d√º≈ü√ºk g√ºvenli sonu√ß filtrelendi")
            self.metadata["total_detections"] = len(self.data)
    
    def remove_duplicates(self, time_threshold: float = 1.0, similarity_threshold: float = 0.9):
        """
        Benzer metinleri temizler
        
        Args:
            time_threshold (float): Zaman e≈üiƒüi (saniye)
            similarity_threshold (float): Benzerlik e≈üiƒüi
        """
        if len(self.data) <= 1:
            return
        
        from difflib import SequenceMatcher
        
        filtered_data = []
        
        for current in self.data:
            is_duplicate = False
            
            for existing in filtered_data:
                # Zaman farkƒ± kontrol√º
                time_diff = abs(current["timestamp"] - existing["timestamp"])
                if time_diff > time_threshold:
                    continue
                
                # Metin benzerlik kontrol√º
                similarity = SequenceMatcher(None, 
                                           current["text"].lower(), 
                                           existing["text"].lower()).ratio()
                
                if similarity >= similarity_threshold:
                    is_duplicate = True
                    # Daha y√ºksek g√ºven skorlu olanƒ± tut
                    if current["confidence"] > existing["confidence"]:
                        filtered_data.remove(existing)
                        filtered_data.append(current)
                    break
            
            if not is_duplicate:
                filtered_data.append(current)
        
        removed_count = len(self.data) - len(filtered_data)
        self.data = filtered_data
        
        if removed_count > 0:
            print(f"üßπ {removed_count} tekrar eden sonu√ß temizlendi")
            self.metadata["total_detections"] = len(self.data)
    
    def finalize(self, include_stats: bool = True, pretty_print: bool = True):
        """
        Veriyi JSON dosyasƒ±na yazar ve i≈ülemi tamamlar
        
        Args:
            include_stats (bool): ƒ∞statistikleri dahil et
            pretty_print (bool): Okunabilir format kullan
        """
        if include_stats:
            self.metadata["statistics"] = self.get_statistics()
        
        # Son metadata g√ºncellemeleri
        self.metadata["completed_at"] = datetime.now().isoformat()
        self.metadata["output_file"] = os.path.abspath(self.output_path)
        
        # JSON yapƒ±sƒ±
        output_data = {
            "metadata": self.metadata,
            "detections": self.data
        }
        
        # Dosyayƒ± yaz
        with open(self.output_path, "w", encoding="utf-8") as f:
            if pretty_print:
                json.dump(output_data, f, ensure_ascii=False, indent=2, sort_keys=True)
            else:
                json.dump(output_data, f, ensure_ascii=False, separators=(',', ':'))
        
        print(f"üíæ JSON dosyasƒ± kaydedildi: {self.output_path}")
        
        # √ñzet bilgi g√∂ster
        if self.data:
            stats = self.get_statistics()
            print(f"üìä Toplam tespit: {stats['total_detections']}")
            print(f"‚è±Ô∏è  Zaman aralƒ±ƒüƒ±: {stats['time_range']['duration']:.2f}s")
            print(f"üéØ Ortalama g√ºven: {stats['confidence_stats']['avg']:.3f}")

    def _format_mmss(self, seconds: float, show_ms: bool = False) -> str:
        """Saniyeyi [MM:SS] formatƒ±na √ßevirir."""
        if seconds is None:
            return "[00:00]"
        total_ms = int(round(seconds * 1000))
        mm, ss = divmod(total_ms // 1000, 60)
        if show_ms:
            ms = total_ms % 1000
            return f"[{mm:02d}:{ss:02d}.{ms:03d}]"
        return f"[{mm:02d}:{ss:02d}]"

    OUTPUT_DIR = r"C:\Users\ASUS\Desktop\videotext\processor\output"

    def export_grouped_text(self, text_file_path: str = None, sep: str = " | ", show_ms: bool = False):
        """
        Aynƒ± zamana ait tespitleri tek satƒ±rda birle≈ütirir:
        [MM:SS] - TXT1 | TXT2 | ...
        """
        # Yol belirtilmediyse veya sadece dosya adƒ± geldiyse output klas√∂r√ºne ata
        if not text_file_path:
            text_file_path = self.output_path.replace('.json', '_frames.txt')
        elif not os.path.isabs(text_file_path):
            text_file_path = os.path.join(OUTPUT_DIR, text_file_path)

        os.makedirs(os.path.dirname(text_file_path), exist_ok=True)

        from collections import OrderedDict
        buckets: "OrderedDict[float, list[str]]" = OrderedDict()

        for d in sorted(self.data, key=lambda x: x["timestamp"]):
            ts = d["timestamp"]
            txt = d["text"].strip()
            if not txt:
                continue
            if ts not in buckets:
                buckets[ts] = []
            buckets[ts].append(txt)

        with open(text_file_path, "w", encoding="utf-8") as f:
            for ts, texts in buckets.items():
                stamp = self._format_mmss(ts, show_ms=show_ms)
                line = f"{stamp} - {sep.join(texts)}"
                f.write(line + "\n")

        print(f"üìÑ Kare bazlƒ± metin dosyasƒ±: {text_file_path}")

        
    
    def export_text_only(self, text_file_path: str = None):
        """
        Sadece metinleri basit txt dosyasƒ±na yazar
        
        Args:
            text_file_path (str): Metin dosyasƒ± yolu (opsiyonel)
        """
        if not text_file_path:
            text_file_path = self.output_path.replace('.json', '_texts.txt')
        
        with open(text_file_path, "w", encoding="utf-8") as f:
            for detection in sorted(self.data, key=lambda x: x["timestamp"]):
                timestamp = detection["timestamp"]
                text = detection["text"]
                confidence = detection["confidence"]
                
                f.write(f"[{timestamp:7.2f}s] {text} (g√ºven: {confidence:.3f})\n")
        
        print(f"üìÑ Metin dosyasƒ±: {text_file_path}")


    